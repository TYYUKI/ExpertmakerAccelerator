\documentclass[a4paper]{article}

\usepackage[pdftex]{graphicx,color}  % ab: for xfig pdf export

% minted
\usepackage{minted}
\usemintedstyle{colorful}
\definecolor{bg}{rgb}{0.95,0.95,0.95}
\definecolor{bg_shell}{rgb}{0.95,0.95,1.00}
%\newminted[python]{python}{bgcolor=bg, frame=lines}
\newminted[python]{python}{}
%\AfterEndEnvironment{minted}{\par\noindent}
%\makeatletter
%\patchcmd{\minted@colorbg}{\noindent}{\medskip\noindent}{}{}
%\apptocmd{\endminted@colorbg}{\par\medskip}{}{}
%\makeatother




%\addtolength{\topmargin}{-2cm}
%\addtolength{\textheight}{2cm}
%\addtolength{\textwidth}{1cm}
%\addtolength{\oddsidemargin}{-0.5cm}



\title{The Expertmaker Accelerator\\[1ex]\large{Processing one Billion Lines of Data per Second on a
  Single PC}\\\Large{--- Abstract ---}}

\author{A.\,Berkeman, C.\,Drougge, and S.\,H\"orberg}
\date{}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{figure}[h]
  \begin{center}
    \input{figures/test.pdftex_t} %the difference is just this part
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}



% Your 1-page abstract should include:

% Motivation: Why do we care about the problem and the results?

% Problem statement: What problem are you trying to solve? What is
%   the scope of your work (a generalized approach, or for a specific
%   situation)?

% Approach: How did you go about solving or making progress on
%   the problem? Did you use simulation, analytic models,
%   prototype construction, or analysis of field data for an
%   actual product? What was the extent of your work (did you look
%   at one application program or a hundred programs in twenty
%   different programming languages?) What important variables did
%   you control, ignore, or measure?

% Results: What's the answer?
%   Conclusions: What are the implications of your answer? Is
%   it going to change the world (unlikely), be a significant
%   ``win'', be a nice hack, or simply serve as a road sign
%   indicating that this path is a waste of time (all of the
%   previous results are useful). Are your results general,
%   potentially generalizable, or specific to a particular
%   case?
            

\emph{
 teaser om hur coola grejer man kan göra innan vi börjar med triviala
 exempel...  ``Så, för att förstå hur det är möjligt måste vi börja
 med...'', typ
}\\

\emph{Maste namna workspace eller om det heter workdir}\\

\emph{Maste namna python}\\


\subsection*{What It Is}
The Expertmaker Accelerator is a tool for processing big data.  It has
a small footprint and low overhead, and provides a number of
interesting features, most notably very fast data access and a novel
scheme to store, retrieve, and reuse computations.  It has been used
in commercial projects since 2012.

\subsection*{Background}
vad den anvants till, hur den utvecklats osv

\subsection*{Motivation and Approach}
Processing big data is potentially both time consuming and resource
hungry.  This is addressed in the Accelerator design by the following
two principles:
\begin{itemize}
\item[1.] Modern computers are very powerful.\\ The Accelerator has
  very vast data streaming from disk to CPU cores.  Critical parts are
  written in the C programming language, and effort has been spent to
  come close to the theoretical performance limits set by modern
  hardware.
  
\item[2.] Results are valuable, they should be easy to retrieve and
  re-use.\\ The Accelerator connects each computed result to the
  source code that was running, the input data used, and the set of
  input parameters.  This connection makes it possible for the
  Accelerator to automatically retrieve results instead of
  re-computing them, which saves time.  It also brings full
  transparency in that all work can be trivially traced back to its
  origins.
\end{itemize}



















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Functionality: Jobs}
The basic operation of the Accelerator is to execute small programs
called \textsl{methods}.  A method is, simply put, a python program
that instantates one or more special functions.





\subsection{Basic Job Running:  ``Hello, World''}



Let's begin with a simple ``hello world'' program.  We create a method
with the following contents
\begin{python}
def synthesis():
    return "hello world"
\end{python}
This method takes no inputs, it just returns a string and then
exits.\footnote{This program does not print anything to
  \texttt{stdout}.  Printing to \texttt{stdout} an so on is not
  altered by the Accelerator, so for example \texttt{print()} will
  work as expected.}  The Accelerator itself only returns a unique
identifier, called a \textsl{jobid} back to the user.  This jobid can
be mapped to a directory where the output from the run is stored
together with all information that was needed to run the job and some
profiling information.

Among the information stored in the job direcory is a hash digest of
the method's source code.  This is used to figure out if a result
could be re-used instead of re-computed.  To get an idea of how this
works we could try to run the \texttt{hello\_world} method again.
Interestingly, it will not execute.  The Accelerator knows that the
job has been run before, and therefore it immediately returns the
jobid to that existing job.

Figure~\ref{fig:execflow-hello-world} illustrates the dispatch of the
\texttt{hello\_world} method.  The created jobid is called
\texttt{job\_0}, and corresponding directory is shown in green.  The
directory contains several files, the most important ones right now
are \texttt{setup.json} which contains job information, and
\texttt{result.pickle} containing the returned data.



\vfill
\begin{figure}[h!]
  \begin{center}
    \input{figures/execflow_hello_world.pdftex_t}
    \caption{A simple hello world program.}
    \label{fig:execflow-hello-world}
  \end{center}
\end{figure}

\clearpage






\subsection{Jobs parameters}

\begin{figure}[t!]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobids},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}

There are three kinds of parameters that can be supplied when
dispatching a method.  These are
\begin{itemize}
  \item[] \texttt{options}, a dictionary of options;
  \item[] \texttt{datasets}, a set of input datasets, explained later; and
  \item[] \texttt{jobids}, a set of identifiers to previously executed jobs.
\end{itemize}
And this is depicted in figure~\ref{fig:execflow}.  Input to a job
dispatch are the parameters and which method to be executed.  Output
is a completed \textsl{job} (a directory with files), or in case of
subjobs, a set of \textsl{jobs}.


\subsection{Jobs in more detail}

Here is what happens when a job is dispatched.
\begin{enumerate}
\item It generates a unique identifier, called a \textsl{jobid}.
\item A directory named after the jobid is created in the target workdir.
\item Some files with job-information is stored in the directory.
\item The method is executed.
\item The running job may create files in the directory
\item After successful completion, profiling information is written
  to the directory.
\end{enumerate}

The uniqe identifier, the \textsl{jobid} is derived from the storage
location, which is denoted a \textsl{workdir}, concatenated with an
integer that increases by one for each new job.  It is possible to
have any number of workdirs, and information in the can be shared
between users, so different users to re-use eachothers successfully
completed jobs.

The files stored in the job directory at dispatch are complete in the
sense that they contain all information required to run the job.  So
the Accelerator job dispatcher actually just creates processes and
points them to the job directory.  Then, the processes have to go
figure out the rest by themselves.  When the job completes, the files
are updated with profiling infomation, such as executin time spent in
single process mode and execution time spent in every parallel
process.  Along with the hash of the method's source, all code that is
directly related to the job running is also stored in a compressed
archive.  This code is typically limited to the method's source, but
the code may have manually added dependencies to any other files, and
in that case these are added too.  This way, source code and results
are always connected and conveniently stored in the same directory for
future reference.

When a job completes, it stores the return value into a file named
\texttt{result.pickle}.  (As a technical side note, the return value
is serialised into Python's pickle format, but a user is not strictly
required to know about the format.)  Any other files created by the
method are also stored in the job directory, since it is the current
working directory of the job's processes.

Finally, after successful execution the Accelerator returns the jobid
to the finished job.

To repeat, if the Accelerator is asked to execute the same
\texttt{hello\_world} method again, it will immmediately return the
previous jobid \textsl{instead of executing the method again}.
Internally, the Accelerator checks for previous jobs that run the
\texttt{hello\_world} method, checks that they have the same parameter
set, and checks that the hash digest of the method source is the same.
If a match is found, it returns the corresponding jobid.  If not, the
method will be executed because it is somehow differs from previous
runs.

The behaviour of storing previously run jobs and being able to
retrieve them very quickly is indeed useful, as we will see next.



\subsection{Linking Jobs}
Assume that the job that we just run, \texttt{job\_0} based on the
\texttt{hello\_world} method, actually did something computationally
expensive, and we want to continue by generating some output based on
this work.  To keep things simple, we print the result to
\texttt{stdout}.  So, in order to do this, we create a new method that
we call \texttt{print\_result}, and it goes like this

\begin{python}
import blob
  
jobids = {hello_world_job,}

def synthesis():
    x = blob.load(jobid=jobids.hello_world_job)
    print(x)
\end{python}

This method expects a jobid associated with the key
\texttt{hello\_world\_job} is input at execution time.  (The
\texttt{jobids} parameter is actually a set, so any number of jobids
could be included.)  During execution, it will read the results from
the jobid and assign it to the variable \texttt{x}.  The variable
\texttt{x} is then printed to \texttt{stdout}.  This method does not
return anything at completion time.
Figure~\ref{fig:execflow-print-result} illustrates the situation.

\begin{figure}[h!]
  \begin{center}
    \input{figures/execflow_print_result.pdftex_t}
    \caption{The jobid of the first job, \texttt{job\_0}, is used as
      input to the \texttt{print\_result} job.}
    \label{fig:execflow-print-result}
  \end{center}
\end{figure}

Now that we have seen how the jobid of a previous job is
fed into the execution of a new job, we can omit some detail in the
figures and redraw them like in figure~\ref{fig:depflow-twojobs}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/depflow_twojobs.pdftex_t}
    \caption{Job dependencies.  \texttt{job\_1} points to
      \texttt{job\_0}.}
    \label{fig:depflow-twojobs}
  \end{center}
\end{figure}

\noindent Note the direction of the arrow.  The second job,
\texttt{job\_1} is aware of \texttt{job\_0}, since the jobid of the
latter was input when executing \texttt{job\_1}.  The first job,
\texttt{job\_0}, does not know of any jobs made in the future.


\subsection{Advantages..}
Let's have a short summary of what we've seen so far and what it
means.  Job linking brings a number of advantages, of which two really
stand out.
\begin{itemize}
\item[] We can rely on previous results to be available immediately
  upon request.  This means that we can speed up development and
  execution time, since we do not ever need to recompute any results.
  This is particularly advantageous when designing incrementally, one
  job at a time.
\item[] All previous results are stored and bookkept, so there is a
  full log of which job that was run when, which input data it used
  and what the job options were.  No need for any detective work
  again, never ask yourself the question ``what data is this plot
  based on'' and so on.
\end{itemize}

\subsection{Working with data: Datasets}
Now we will turn our focus to the \texttt{dataset}, which is a very
efficient way to deal with small and large sets of data.  Datasets are
stored in jobs.  A job could have any number of dataset, but we will
start by looking at a simple example with one dataset, the import of a
file.  See figure~\ref{fig:dataset_csvimport}

\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_csvimport.pdftex_t}
    \caption{caption here}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}

\noindent In this example, the method to run is \texttt{csvimport},
which is a standard method supplied with the Accelerator.  This method
takes a mandatory option specifying the filename of the input data to
import.  The data is imported into a dataset that is stored in the
job, in this case \texttt{job\_0}.  If no name is specified, the
dataset will be named \texttt{default}, and the full reference to the
dataset is \texttt{job\_0/default}, or \texttt{job\_0} for short.

A key feature is that the datasets can be linked to eachother, much
like the way we have just seen jobs may link to one another.  We've
just imported \texttt{file1.txt}.  Assume now that there is more data
stored in \texttt{file2.txt}.  We import this data too, and add a
\texttt{dataset} reference to the import of the previous file, like in

\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_csvimport_chain.pdftex_t}
    \caption{caption here}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}

\noindent If we look at the datasets only we have a situation like in
figure~\ref{fig:dep_dataset_csvimport_chain}.  The second import was
fed with the input dataset from \texttt{job\_0}, and this link is
represented by the arrow in the image.

\begin{figure}[h!]
  \begin{center}
    \input{figures/dep_dataset_csvimport_chain.pdftex_t}
    \caption{caption here}
    \label{fig:dep_dataset_csvimport_chain}
  \end{center}
\end{figure}

\noindent The \texttt{job\_1} dataset reference can now be used to
access all data imported from both files.

Dataset chains are particularly convenient when dealing with data that
grows over time.  Typically, this is log data, and it could be a log
of transactions, user interactions, etc.

\subsection{Multiple datasets in a job}
Datasets live inside jobs, and it does not really matter if all
datasets are in one job, or if there is one job for each dataset.
Typically, data is imported one file at a time, and then there is one
job per dataset, but sometimes this data is transformed into something
else, and then it is practical to keep a large number of datasets, but
storing them all in a single job.

Let's look at a situation where it is convenient to create multiple
datasets in a job.  Assume we have a \texttt{dataset} with a column
that holds boolean values, and that we want to separate the set into
two disjoint sets based on the value of the boolean.

\begin{figure}[h!]
  \begin{center}
    \input{figures/filter_dataset.pdftex_t}
    \caption{caption here}
    \label{fig:dep_dataset_csvimport_chain}
  \end{center}
\end{figure}

The figure shows a job that has taken a previous job's
\texttt{default} \texttt{dataset} as an input, and based on that data
created two new datasets, named \texttt{True} and \texttt{False}.  A
third job is using the \texttt{True} part as input.  Short repetition
on the direction of the arrow.  \texttt{job\_1} filtered the
\texttt{dataset} from \texttt{job\_0}.  So if we take a look at
\texttt{job\_1} and wonder where the input data is, we just follow the
link back to \texttt{job\_0}.  If this job is an import, it will hold
the name of the file that was imported.  If not, it will link back to
another job, and so on, all the way down to the source of the dataset.
So there is full tracking and easy observability of everything that
relates to job execution in the Accelerator.


This example actually touches again on something that is important.
Assume that we have a large dataset and that we want to do complex
operations on it.  Part of these operations uses only a minor part of
the data.  If this set of data is separated into its own
\texttt{dataset}, a lot of time might be saved when performing
operations on it, due to its smaller size.  (As we will se further on,
the Accelerator is really fast when it comes to reading datasets and
conditionally skip lines.)  By storing the \textsl{other} data as
well, we keep an copy of all the data, and we can chose to either
process that chunk only, or run on all data, either from the original
dataset or by iterating over both the \texttt{True} and \texttt{False}
datasets.



\subsection{Understanding Datasets}
A \texttt{dataset} is a storage of a matrix of data.  It has a number
of rows and a number of columns.  The columns have names and types.
In order to have fast parallell execution, the columns are split into
a fixed number of \textsl{slices}.



\subsection{Adding New Columns to the Datasets}
So far, we have seen how simple it is to add more lines of data to a
\texttt{dataset} using chaining.  Now we will turn our attention to
creating new columns in an existing dataset.  This is an important and
common operation that the Accelerator handles very efficiently in
linear time.\\
\textsl{before we go on we need to understand a little bit of the dataset internals}\\

The idea is very simple.  Assume that we have a \texttt{dataset} to
which we want to add a new column.  What we do is that we create a new
dataset with \textsl{only} the new column, and while creating that
\texttt{dataset} we instruct the method to linke the job to the
existin \texttt{dataset} with the original columns.  Accessing the new
\texttt{dataset} will seamlessly read columns from the two involved
datasets to make it indistinguishable from a single dataset.  See
figure.



\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{caption here}
    \label{fig:dep_dataset_csvimport_chain}
  \end{center}
\end{figure}








\section{Accessing Datasets}
Accessing a \texttt{dataset} in a method is simple.  Assume that we
have a \texttt{dataset} with a column named \texttt{title}, and we
want to know the ten most common titles.  Consider the following
complete code example
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(title for title in datasets.source.iterate(None, 'title'))
    print(c.most_common(10))
\end{python}

\noindent This will print the ten most common titles and their
frequencies in the \texttt{source} \texttt{dataset}.  The code in the
example above runs serially on a single CPU.  We may parallellize it
like this
\begin{python}
def analysis(sliceno):
    return Counter(title for title in datasets.source.iterate(sliceno, 'title'))

def synthesis(analysis_res)
    c = analysis_res.merge_auto()
    print(c.most_common(10))
\end{python}

\noindent In this example, we run the \texttt{dataset} iterator in the
\texttt{analysis} function, which is forked into a pre-determined
processes when invoked.  The parameter \texttt{sliceno} is an integer
holding the unique number for each process.  Each process stores its
result, and the single process in \texttt{synthesis} merges all
results into one.  That's it.

\subsection{More on the Iterator}
The iterator can run on chained datasets and there are various stop
conditions: stop after a certain number of datasets, stop at a certain
dataset, stop at another jobs input parameters.

It is possible to iterate over a range of lines in a dataset

hashlabel
translators/filters
callback
multi-user
urd - how?

rehasha dataset istället för enorm reduce!


dataset\_separate\_by\_bool i agg-projektet.  Fundera på om den kan
generera dataset-chains ut också.

fundera på bilder med kataloger i text istället för ovaler etc.

\clearpage

\begin{figure}[h!]
  \begin{center}
    \input{figures/dirtest.pdftex_t}
    \caption{caption here}
    \label{fig:dep_dataset_csvimport_chain}
  \end{center}
\end{figure}






Let's start with a very simple example.  Assume that we are to analyse
a system's log files.  The log files are in CSV-format, and there is a
new log-file present every hour.  For the case of simplicity, assume
the files are named like this

\begin{tabular}{l}
  \texttt{log\_03:00.txt}\\
  \texttt{log\_04:00.txt}\\
  \texttt{log\_05:00.txt}\\
  $\dots$
\end{tabular}

\noindent The first step would be to import the files.  This is done
using the \texttt{csvimport} method.  This method takes an input
filename as an option.  Optionally, it can also take the jobid of a
previous job as input.  Using a simple for-loop, we can have each
csvimport fed with the jobid of the previous import job, like in
figure.
\begin{figure}[h!]
  \begin{center}
    \input{figures/csvimport_chain.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}
The figure shows three jobs, \texttt{job\_0} to \texttt{job\_2}, each
of which has executed the \texttt{csvimport} method.  If we loop at
the arrows for a minute, we see that for example \texttt{job\_2}
points to \texttt{job\_1}.  This means that \texttt{job\_2} is aware
that \texttt{job\_1} is its \textsl{previous} job.  In all, there are
three jobs in this \textsl{chain}.  We can also see that
\texttt{job\_1} points to the input file \texttt{log\_04:00.txt},
which means that the job is aware that this is the filename that was
input to the method when executed.  Note the direction of the arrows.
By looking at a single job, we can follow the arrows out and see what
the job is aware of/linked to.

Now, we're ready to do some analysis.  We want to analyse the latest
data, so we run our \texttt{log\_analysis} method with the latest
import job as input, see figure.
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_analysis.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}
The analysis job gets jobid \texttt{job\_3}, and it is aware of
\texttt{job\_2}, which is the last imported log file.

Inside the analysis method, there is a choice of how to iterate the
input data.  It can either loop over the data stored in
\texttt{job\_2}, which is the only data-containing job it is aware of,
or it can iterate over the whole chain of datasets endind at
\texttt{job\_2} all the way back to \texttt{job\_0}.  It is also
possible to iterate over some of the jobs in the chain.

In the figure, we can also see markers in blue of the form
\texttt{[name@timestamp]}.  These are storage markers used by the
\textsl{Urd} job tracking system.  Urd is something like a database
storing information about all jobs being executed on the system.  In
this case, there is a set of jobs tagged with \texttt{import} for
three different timestamps (\texttt{03:00}, \texttt{04:00},
\texttt{05:00}).  There is also a tag \texttt{analysis} with timestamp
\texttt{05:00}.  These tags are really useful when it comes to
tracking jobs, input data, and results, as we will see shortly.

Now, let us assume that two hours have passed, and that two new log
files have appeared.  We want to know what the analysis says based on
these new files, so we import them and run analysis again, like in
figure.
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_analysis_aga.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}
We now have in total seven jobs, where the new analysis job with jobid
\texttt{job\_6} is the latest.

Assume now that we keep the analysis results for the two different
analysis-job runs, and then forget all about this.  Here are examples
of a few questions that may rise some time later when looking back at
the analysis results

\begin{itemize}
\item[]``This result, how old is it?'',
\item[] ``whas it the 05:00-run or the 07:00-run?'', or
\item[] ``which log files are actually used in the 03:00-run?''
\end{itemize}

You know the feeling.  Fortunately, the Urd and the Accelerator has
kept book of everything related to these jobs, so a first thing to do
is to lookup which analysis jobs that has been run in Urd.  A simple
query will tell that two analysis-jobs are stored, one with timestamp
\texttt{05:00}, and one with \texttt{07:00}.  But there is more to it.
It will also tell you that for a specific job, say the latter, it
depends on the import job that is timestamped \texttt{07:00}.  Then
you can proceed and look at this import job, to find that it has file
\texttt{log\_07:00.txt} as input.  Now we are 100\% certain what the
analysis job at \texttt{07:00} was about.

We can also do other things.  Say that we want to do a what-if test on
the first analysis job.  We have changed the code a little bit, and we
want to try out these changes in an \textsl{equivalent} environment.
This is simple, just fetch the dependencies of \texttt{analysis@05:00}
and feed to a new job running the analysis method.  After execution,
it will look like in figure.
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_analysis_aga_rerun.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}
A new job \texttt{job\_7} has been created.  We have given it a new
tag \texttt{analysis-test} so that it does not clash with the original
analysis jobs.  A clash would not have caused any problems in the
system, though.  Urd and Accelerator would be able to tell them apart.
But for human readability and ease of understanding we change the
name.  Now we can switch back and forth between the two jobs and see
what implications a code change might have.


\subsection{A slightly more advanced example}
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_learn_publish_validate.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}




\subsection{gurk}
\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_configurations.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}

\subsection{gurk}
\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_configuration_chain.pdftex_t}
    \caption{caption here}
    \label{figure:example}
  \end{center}
\end{figure}



















In this case that makes sence, since there are several files and they
are connected in time.

Since there are several files, and they are
connected in time, it makes sense to import them as a \textsl{chain},
i.e.\ 


The data files are log files in CSV-format, and there is a new log
file written every hour.  The filename of each file reflects the date
of the corresponding data.




Assume that we have access to a number of data files,

Let us start with a simple example, importing a text file.







\subsection*{Function}


\subsection*{Results}
The Accelerator has been proven in a number of commercial projects.
At eBay it is currently used to aggregate products from listings.
Here are some performance figures
\begin{itemize}
\item[1.]  \textbf{Sum all values in a column:\hfill 1000 million rows/second.}\\
  Adding up all transaction amounts.  \textsl{All} of eBay's
  transactions added in 17 seconds.
\item[2.]  \textbf{Count unique strings\hfill  300 million rows/second.}\\
Count all 150 million MPNs (a product identifier) in a dataset of 6.3
billion rows.
\item[3.]  \textbf{Complex append a column.:\hfill 80 million rows/second}.\\
  For each row, \textsl{read} three columns, multiply their values,
  and \textsl{write} to a new column (on disk).
\end{itemize}

\noindent These results are achieved on a single Krylov-type instance with
72~CPU cores and 1TB RAM.




% \clearpage

% What it is

% \subsection*{Job Recording}

% The Accelerator will remember all computed results.  There are two
% main reasons for this.  The first is that old results can be retrieved
% easily instead of being recomputed, which saves time and helps
% structuring analysis work results.  The second reason is that this
% encourages iterative development, where each new job depends on the
% results of one or more previous jobs.  Apart from being an attractive
% and fast development strategy, this helps in tracking and debugging
% and really speeds things up.

% Result storage is achieved by recording the connections between
% computed results and everything that was input to the computation,
% i.e.

% \begin{centering}
% result $\leftarrow$ data + source code + options + execution\_time\\
% \end{centering}

% If the same combination of input conditions are presented to the
% Accelerator again, it will check if there is a connection to a
% successfully completed job, and if there is it will return a pointer
% to that job's output.

% \subsection*{Fast streaming}
% The Acceleator stores data very efficiently in datastructures called
% \texttt{datasets}.  These are optimized for very fast streamed reading
% and writing.  Streamed data access is different from traditional
% database random access, and yields much higher performance in
% applications where a significant part of all data is being processed.
% The dataset optimizes for throughput, minimized number of disk seeks,
% and zero-overhead append of rows and columns.



% What it may be used for.


% The Accelerator connects a computed results to the source code used,
% the input data, and input options.  A job will be run only once, so
% trying to re-execute something that has completed successfully will
% result in the immediate return of the result computed the first time.



% The Accelerator is a tool for processing big data that has been in
% commercial use since 2012.  It is designed for both analysis and
% production work.  Key features include
% \begin{itemize}
% \item[] Very high speed data streaming from storage to CPUs.
% \item[] Storage and recall of all previously successfully computed jobs.
% \end{itemize}
% In combination, these features are extremely useful for both analysis
% and production tasks.  Storage and recall means that all computations
% are traceable, so that every computed result could be followed back to
% all used input data.  This is useful in production for debugging and
% validation.  It is also very useful in analysis work, since results
% are automatically connected to the input data and source code that was
% used when computing the result.  This automatic connection of results,
% code, and input data eliminates the need to manually storing under
% what conditions a result was maintained.  Also, a change of any input
% data or source code will automatically make the computations affected,
% and only those, by the change to be recomputed.

\end{document}
