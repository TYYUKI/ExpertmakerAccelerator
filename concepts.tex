
\section{Introduction}

The accelerator environment is composed of two parts: a server
providing a set of features for efficient processing of large amounts
of data, and a client that dispatches jobs on the server.

A user may use or write two kinds of programs for the enviromnent.
the first is called methods, which is parallel programs executed by
the server.  The second is high level scripts, called automatas, that
are executed by the client.  Automatas are used to control flow and
parameters to and from the methods running on the server.

Programs are written in the Python programming language, both python 2
and 3 are supported.  In theory, any language may be used.

Methods that have been running are called jobs.  The server stores
results and parameters from all jobs in a database that is partitioned
into what is called workdirs.  The client keeps a log, called the urd
log, which is bookkeeping all dispatched jobs and parameters.

@@@ featurelist here?!

A very simple example

\begin{python}
# a_helloworld.py:
def analysis(sliceno):
  print("Hello world: %d" % (sliceno,))

# automata.py:
def main(urd):
  urd.begin('test')
  urd.build('helloworld')
\end{python}


\section{Job Bookkeeping}

A promiment feature of both the client and the server is the ability
to immediately recall instead of recompute previously executed jobs.
Any job that has been successfully executed may be recalled, provided
that the job is submitted again with exactly the same input and source
code.  The framework uses a hash of the method's source code to decide
if it has changed or not.

The combination of bookkeeping and the client/server architecture
provides a very powerful environment for accessing and depending on
previous result.  Not only can a user's own results be recalled, but
also those from another user or from the ``production'' user in a live
enviroment.

In a typical application, this dramatically increases performance
while minimising the probability of errors.  It also allows for a high
level of transparency.  Past jobs could be replayed in a controlled
manner, and the complete context with respect to input argments of any
job could be retreived and used to answer ``what-if''-scenarios.

@@ Jag vill inte ha exempel, utan harda definitioner!



\section{Urd}

Urd provides a scripting environment for running and recalling jobs.
It has a transaction-log database that stores information about jobs
that have been run and their inter-dependencies.

Urd is designed to answer questions like: ``what is the jobid of the
job that ran customer recommendations on this and this date'', and
``what data did that recommendation job depend on, which was the
latest available transaction log at that time''.




\section{Workdirs}

A workdir is where results are stored.  One can think of it as a
directory populated with jobid-directories.  Workdir are specified
with absolute paths in the server filesystem.  A workdir has a fixed
slicing.  All workdir in a project need to have the same number of
slices.



\section{Dispatch, Dependencies, and Jobids}

The framework is basically a job dispatcher and depencency tracker.
Input to a job is parameters and source code, called method.  When a
job is run, a directory is created in the workdir.  A pointer to this
directory, called a jobid, is returned upon finished execution.
Inside the directory, everything needed to run the job is stored, for
example input parameters, as well as all results created during
execution.

The framework keeps information for each job that has been
successfully completed for dependency tracking.  Dependency tracking
is used for two things: first, it avoids re-execution of jobs that
have been run before; and second, it permits jobs to depend on already
finished jobs.

Re-execution is avoided if a job is issued with exactly the same
source code and input options.  If source code is not equal, or if any
input to the job is different, the job will be executed again.  All
successful runs of a job will be kept.

A job may have reference to other jobs as input.  If these references
are valid, the running job will have access to all information from
the reference jobs.



\section{Datasets and Chaining}

A particularly efficient way to store data is by using the dataset.
Data is stored so that it can be retrieved row by row at a very high
pace and requesting from one to any number of columns.

Data in a dataset is separated into slices, where on a single server,
the number of slices is close to the number of actual CPU cores for
best performance.

A dataset may be hashed on one of its columns.  Hashing means that
each slice contains a mutually exclusive set of column data.

Typically, a dataset is created for each file that is imported.  When
several files are imported, the jobids may be chained, that is, each
new jobid contains a pointer to the previous job.  Using the
previous-pointers, datasets could be iterated one after another
without interruption.



\section{Method}

Code is executed on the framework is written in methods.  Methods
allow for parallel execution and more.

\section{Automata}

