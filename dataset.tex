\section{Introduction}
The dataset is the prefered way to store large amounts of data.  The
dataset is the container for fast and simple access to data.  Data in
a dataset are stored as a matrix in rows and columns.  Using the
dataset, data is simple to access and at a very high performance.

The dataset is implemented as a special type of job, and any method
could create a dataset.  The most obvious use of a dataset is the
cvsimport method that creates a dataset from an input file.

A job may contain any number of datasets.  This is convenient in for
example filtering jobs, where data could be split into two or more
datasets depending on the filtering function.

The dataset class is also a string, which is either ``jobid'' if only
one dataset (and name is default), or ``jobid/name'' when more than
one dataset is stored.







\section{Dataset properties}


\begin{python}
datasets = ('source',)

def synthesis():
\end{python}

\begin{python}
  print datasets.source.columns.keys()
  # [u'GTIN', u'date', u'locale', u'subsource']

  # each key, i.e. column, has a number of properties, of which the
  # most important ones are shown below
  print datasets.source.columns['locale'].type
  # ascii
  print datasets.source.columns['locale'].name
  # locale
  print datasets.source.columns['locale'].min
  # 3
  print datasets.source.columns['locale'].max
  # 9
  
\end{python}

Will procuce a vector of the number of lines in each slice, like this

\begin{python}
  print datasets.source.lines
  """
[5771L, 6939L, 6212L, 6312L, 6702L, 6341L, 5988L, 6195L,
 6741L, 6587L, 6518L, 5840L, 6327L, 5933L, 6745L, 6673L,
 6536L, 6405L, 6259L, 6455L, 6036L, 6088L, 6937L, 6245L,
 6418L, 6437L, 6360L, 6106L, 6878L]
"""
\end{python}

A tuple of number of columns and total number of lines

\begin{python}
  print datasets.source.shape
  # (4, 184984L)
\end{python}
the second number is exactly the sum of the number of lines for each
slice from above.

other properties are

\begin{python}
  print datasets.source.filename
  """
/data/incoming/raw_repository_5391.gz
"""
\end{python}

\begin{python}
  print datasets.source.caption
  """
flattening
"""
\end{python}


and more

\begin{python}
  print datasets.source.hashlabel
  """
GTIN
"""
\end{python}
\begin{python}
datasets.source.column\_filename
\end{python}

\begin{python}
  print datasets.source.previous
  """
neu4-4893_0/default
"""
\end{python}




\section{Dataset Iterators}

\subsection{iterate}
\begin{python}
# def iterate(self, sliceno, columns=None, hashlabel=None, filters=None, translators=None):
  for item in datasets.source.iterate(sliceno=None, columns=None):
    print item
\end{python}
sliceno=None iterates over all slices.
columns=None iterates over all columns.

\subsection{iterate\_chain}
\begin{python}
# def iterate_chain(self, sliceno, columns=None, length=-1, reverse=False, hashlabel=None, stop_jobid=None, pre_callback=None, post_callback=None, filters=None, translators=None):
\end{python}



\subsection{iterate\_list}
\begin{python}
  # def iterate_list(sliceno, columns, jobids, hashlabel=None, pre_callback=None, post_callback=None, filters=None, translators=None):
  from dataset import Dataset
  for item in Dataset.iterate_list(None, None, datasets.source):
    print item
\end{python}

\subsection{filters and translators}

\subsection{pre and post callback}

\section{Create New Dataset}
Datasets are either created in prepare + analysis, or in just
synthesis.

\subsection{Create in prepare + analysis}
A simple example writing three columns to the default dataset

\begin{python}
from dataset import DatasetWriter
datasets = ('previous',)

def prepare():
  dw = DatasetWriter(
    hashlabel = 'X',
    previous = datasets.previous,
  )
  dw.add('X', number)
  dw.add('Y', number)
  dw.add('Z', number)
  return dw

def analysis(sliceno, prepare_res):
  dw = prepare_res
  ...
  dw.write(x, y, z)
\end{python}
Note that the order of the variables in the dw.write function call is
the same as the order of the add calls in prepare\footnote{in case
  write is called with a dict, the order is unknown, but then names
  are looked up using the dict keys.}.

DatasetWriter takes a number of optional arguments such as caption and
filename.  The argument ``name'' specifies the name of the dataset,
which is set to be ``default'' when unassigned.  Several datasets can
be created in the same method using more than one datasetwriter
instance with different ``name''s.

There is some flexibility in the way the write function may be called

\begin{python}
  dw.write_dict({column: value})
  dw.write_list([value, value, ...])
  dw.write(value, value, ...)
  # or even
  dw.writers[name].write(value)  # return True if hashed to correct slice
\end{python}


\subsection{Creating Hashed datasets}
If hashlabel is set, one can use dw.hashcheck(value) to check if value
belongs to the slice.  It is also possible to just call the writer
since it will discard anything not belonging to the correct slice.



\subsection{Create in synthesis}

There are two options if the dataset is to be created in synthesis.
One in to set the slice number first

\begin{python}
  dw.set_slice(sliceno)
\end{python}
while the other is to use one of these functions

\begin{python}
  dw.get_split_write_dict()({column: value})
  dw.get_split_write_list()([value, value, ...])
  dw.get_split_write()(value, value, ...)
\end{python}
that will assing the data to the correct slice automatically.

\subsection{Placeholder:  Creating datasets more manually}


\section{Appending to a Dataset}
\begin{python}
datasets = ("source", "previous",)

def prepare():
  dw = dataset.DatasetWriter(
    parent=datasets.source,
    previous=datasets.previous,
    caption="flattening_attempt_1"
  )
  dw.add(name, type)
  return dw

def analysis(sliceno, prepare_res):
  dw = prepare_res
  dw.writer[name].write('x')
\end{python}



% more on datasets: previous, link_to_here
  
