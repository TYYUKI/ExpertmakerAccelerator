%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Iterators are members of the \texttt{dataset} class.  They are
available in methods for streaming dataset data one row at a time into
a program.

The iterator iterates over one or more specified data columns.  Each
output is a tuple corresponding to the specified columns for one data
row.  In case of iterating over a single column, the output may
optionally be a scalar for more efficient computing.  Iterators can be
parallel, in \analysis, or sequential, in \prepare or \synthesis.
There are two iterators available,
\begin{itemize}
\item [] \texttt{iterate()}, for  single dataset iteration, and
\item [] \texttt{iterate\_chain()} for iterating over dataset chains.
\end{itemize}
Both iterators share these arguments
\begin{snugshade}
\begin{tabular}{llp{6cm}}

  \texttt{sliceno} & \textsl{mandatory} & Slice number to iterate
  over, or \mintinline{python}/None/ to iterate over all slices
  sequenctially. \\

  \texttt{columns} & None & Tuple of column labels or a single name if
  iterating over one column.\\

  \texttt{hashlabel} & None & Name of hash column.  If the code relies
  on a dataset being hashed on a particular column, set this to make
  the iterator verify that it is the case.  Execution will terminate
  if the hashlabel is incorrect.\\

  \texttt{rehash} & False & Setting this to \mintinline{python}/True/
  will rehash the dataset on the fly based on the \texttt{hashlabel}
  column.  Ideally, rehashing should be made using the
  \texttt{dataset\_rehash} method~\ref{xx}.\\
  
  \texttt{filters} & None & Filters decide which rows to include.
  Explained in section~\ref{xx}\\

  \texttt{translators} & None & Translators transform data values.
  Explained in section~\ref{xx}\\
  
  \texttt{status\_reporting} & True & Give status when pressing
  \texttt{C-t}.  Unless manually \texttt{zip}ing iterators, this
  should be set to default \mintinline{python}/True/.  See
  \texttt{dataset.py} for full information.\\
\end{tabular}
\end{snugshade}
\comment{heter det columns här men labels överallt annars?  Var heter det labels förutom hashlabel?}
In addition, \texttt{iterate\_chain} takes these arguments too
\begin{snugshade}
\begin{tabular}{llp{6cm}}
  \texttt{length}&$-1$& Number of datasets in a chain to iterate over.
  Default is $-1$, which corresponds to all datasets in a chain.\\
  
  \texttt{range}& None& Filter rows based on a column's value being
  within a range.\\

  \texttt{sloppy\_range}& False & Used with \texttt{range}, but will
  iterate over full datasets for those datasets that have values
  within range.  This might be faster.\\
  
  \texttt{reverse}& \mintinline{python}/False/& Iterate chain
  backwards.  Default is to iterate forward, i.e.\ from oldest to
  newest dataset.\\

  \texttt{stop\_ds}&None& The first dataset older than all datasets in the chain.\comment{duh}\\ 

  \texttt{pre\_callback}&None& A function that will be called before
  iterating each dataset.\\

  \texttt{post\_callback}&None& A function that will be called after
  iterating each dataset.\\
\end{tabular}
\end{snugshade}


\section{Basic Iteration}
Basic use include iterating in parallel or serial over one dataset or
a chain of datasets.

\subsection*{Parallel Iterator Invocation}
For parallel iteration in \analysis, the iterator needs to know the
number of the current slice.  The following is an example of iteration
that happens independently in each slice.

\begin{python}
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate(sliceno, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
The program creates dictionaries mapping \texttt{user}s to sets of
\texttt{item}s for the \texttt{source} dataset.  (If we assume that
the dataset is hashed~\ref{xx}, this operation is entirely parallel
and there is no need to merge all the results from the analysis
processes later.

\subsection*{Sequential Iteratior Invocation}
By specifying the \texttt{sliceno} parameter to
\mintinline{python}/None/, the iterator will run through all slices of
the dataset, one at a time, like in this example
\begin{python}
def synthesis():
    h = defaultdict(set)
    for user, item in datasets.source.iterate(None, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
Slices will be iterated one at at time in increasing order.

\subsection*{Iterate Over Chains}
To iterate over several datasets in a chain, use
\texttt{iterate\_chain}.  The following example will iterate over the
last three datasets in the chain.
\begin{python}
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate_chain(sliceno, columns=('user', 'item',), length=3):
        h[user].add(item)
\end{python}
using \texttt{iterate\_chain} without explicitly specifying
\texttt{length} will default to a \texttt{length} of $-1$, which
corresponds to all datasets in the chain.

Here is an interesting example that will iterate over the datasets in
the chain that are new since the last invocation of the method.
\begin{python}
datasets = ('source',)
jobids = ('previous',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate_chain(sliceno, columns=('user', 'item',), stop_id=jobids.params.source):
        h[user].add(item)
\end{python}
\comment{funkar detta?}  Assuming that \texttt{previous} is a jobid to
the previous invocation of the method, \texttt{jobids.params.source}
will be the input \texttt{source} dataset to that job.  



\subsection*{Special Cases, Iterating Over All or a Singe Column}
It is possible to iterate over all columns in a dataset by specifying
an empty list of column names, like this
\begin{python}
for items in dataset.source.iterate(sliceno, None):
    print(items)  # is a tuple of all columns
\end{python}
The iterator will output a tuples populated with all column values for
each row.  The columns will be in sorted column name order.

Iterators output tuples, but in the case of iterating over a single
column it is more efficient to output scalars instead.  Here are the
two different ways to iterate over a single column
\begin{python}
# alternative 1, use lists/tuples
for user in datasets.source.iterate(sliceno, ('USER',)):
    userset.add(user[0])  # user is a tuple

# alternative 2, specify column as string, not list
for user in datasets.source.iterate(sliceno, 'USER',):
    userset.add(user)
\end{python}
Both styles are supported by filters and translators introduced later
i this chapter.




\section{Halting Iteration}

Iteration over a dataset chain will continue until all data is
exhausted or some stop criteria is fulfulled.  There are several
mechanisms for stopping, and they may be combined in a single
expression.  If so, iteration will be over the shortest range of the
conditions.

\subsection*{Halting Using \texttt{length}}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    length = options.length):
\end{python}
This will iterate for the last \texttt{options.length} number of
datasets.  Note that a length of $-1$ is default and will iterate
without bounds.


\subsection*{Halting Using \texttt{stop\_id}}
Similar to using \texttt{length}, but will stop when reaching a
certain dataset.
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    stop_jobid = datasets.stopjob):
\end{python}

A more advanced, but very useful, method is to stop at a dataset that
is input to another job
\subsection*{Halting Using Another Job's Input Parameters}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    stop_id = {jobids.previous: 'source',}):
\end{python}
This will iterate until reaching end or the dataset
\texttt{job\_params(jobids.preprocess).datasets.source}.



\section{Iterating Over a Data Range}
It is possible to iterate over rows having a specified column's value
within a certain range.  This works best on datasets that a sorted on
the specified column.  \comment{eller ja?}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    range={timestamp, ('2016-01-01', '2016-01-31'),}):
\end{python}
This example will limit the iterator to exactly the range of lines
that fulfill the range condition.  It is relatively costly to filter
each line, and there is a speed advantage by specifying
\texttt{sloppy\_range}, which will iterate over all datasets that
contain part of the range.



\section{Iterating in the Reverse Direction}
By default, iterating over a chain of dataset starts at the oldest
dataset and ends at the latest dataset.  This behaviour can be
reversed by specifying \mintinline{python}/reverse=True/.  But note
that row iteration is always in the forward direction within each
dataset!
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    reverse=True):
\end{python}



\section{Hashed Datasets and Hashing Datasets}
Hashing a dataset on a particular columns, see~\ref{xx} may simplify
the code in methods using the dataset.  This code will not work
properly if it turns out that it is being fed with unhashed datasets.
Therefore, it is a good idea to \emph{assert} the hashlabel by
entering it into the iterator function, like this
\begin{python}
s = {user: item for user, item datasets.source.iterate_chain( \
     sliceno, ('user', 'item',), hashlabel='user')}
\end{python}
Execution will terminate if the hashlabel is not correct.

It is possible to rehash the dataset on-the-fly.  This is done by
setting the \texttt{rehash} argument to the iterator to
\mintinline{python}/True/.  The prefered way to rehash is to use the
\texttt{dataset\_rehash} method~\ref{xx}, since it will store the
rehashed dataset for later use, which in most scenarios will be more
efficient.


\section{Translators}

Translators transform data values. A translator is either a callable or
a \texttt{dict}.  Translators are similar to \texttt{filters}, and
always executed before filtering.

The idea behind translators and filters is that they provide a way to
modify code behavior by supplying functions as iterator options.  In
this way, it is possible to write re-useable simple methods that still
could be altered significantly in behavior.



\subsection*{Callable Translator}

A translator function is called with a tuple and is expected to return
a tuple of the same length.  Using translator functions it is possible
to mix different columns with each other before sending them to the
iterator output.  Here is an example

\begin{python}
def merger(user, item):
    return "%s:%s" % (user, item), None
for merge, _ in datasets.source.iterate_chain(None, ('user', 'item',),
                                     translator=merger):
    ...
\end{python}
The purpose of this translator is to convert each
\texttt{(user, item)} tuple to a string \texttt{user:item}.  This is
the first output of the translator and iterator and is stored in the
\texttt{merge} variable.  The second output variable is not used in
this application, but a variable still has to be assigned.



\subsection*{Translator dict}

One or more columns may be translated independently using a translator
dictionary, specified as \texttt{\{name:\ translation\}}.  A
translation may be either a \texttt{dict} or a callable.  Examples of
both kinds are shown below.

First an example illustrating the use of a translation \texttt{dict}.
Here, integers are translated into more comprehensible strings.

\begin{python}
mapper = {2: 'HUMANLIKE', 4: 'LABRADOR', 5: 'STARFISH',}
for animal in datasets.source.iterate_chain(None, 'NUM_LEGS',
                                     translator={'NUM_LEGS': mapper,}):
    ...
\end{python}
Items missing in a translation-\texttt{dict} yield \texttt{None}.

Second, an example of a translation callable.  In this example, each
user string is output from the iterator reads backwards.

\begin{python}
def reverse(x):
    return x[::-1]
for resu, item in dataset.source.iterate_chain(None, ('USER', 'ITEM',),
                                       translator={'USER': reverse,}):
    ...
\end{python}



%\newpage
\section{Filters}

Filters are run \emph{after} translators.

Filters decide which rows to pass to the iterator output.  A filter is
either a callable or a \texttt{dict}.


\subsection*{Callable Filter}

Callable filters receive the iterator tuple as input.  The output must
be \texttt{True} for the tuple to be output from the iterator,
otherwise the iterator continues reading the next line.

The following two examples, of which the first uses a callable filter,
are functionally equivalent

\begin{python}
# Ex1.  filter users that are bearded and longhaired
filter=lambda line: line[1] and line[2]
for user, beard, hair in datasets.source.iterate_chain(None, ('user', 'bearded', 'longhaired'),
                                       filters=filter):
    ...

# Ex2.  filter users that are bearded and longhaired
for user, beard, hair in datasets.source.iterate_chain(None, ('user', 'bearded', 'longhaired')):
    if beard and hair:
        ...
\end{python}




\subsection*{Filter Dict}

It is possible to filter on one or more columns independently using a
\texttt{dict}.  All filters must be \texttt{True} for a line to be
output from the iterator.  Here are two examples.  The first example
will remove all lines except the ones with valid users.  The second
example will only keep lines with movie items.  (The fact that book is
false is redundant --- missing keys will result in discarded lines.)

\begin{python}
# keep valid users only
validusers={'user1', 'user2', 'user3'}
filters={'user': validusers.__contains__}

# keep valid users with movie items
validitems={'movie': True, 'book': False}
filters={'user': validusers.__contains__, 'item': validitems.get}
\end{python}




\subsection*{Filter by Column Values}


Column values could also be used directly, i.e.\ the values get
evaluated by Python.  For example, assume there is a column
\texttt{hasbeard} with values being boolean integers, i.e.\ \texttt{1}
or \texttt{0}.  Bearded users may be collected by
\begin{python}
for user, beard in datasets.source.iterate_chain(None, ('user', 'bearded',), tip_jobid=jobid,
                                       filters={'bearded': None}):
    bearded.add(user)
\end{python}
This may seem strange at first, but it works because the key
for the \texttt{bearded} column exists, and the value is
\texttt{None}, and not a callable nor a \texttt{dict}.




%You can also pass a single name (a str) as names_list, in which case
%you don't get a tuple back (just the
%values). Tuple-filters/translators also get just the value in this
%case (column versions are unaffected).

    


\section{Callback}
\label{sec:callback}

It is also possible to inject callback functions, either before or
after each dataset is loaded.  If \texttt{sliceno=None},
i.e.\ iteration runs over all slices of all datasets, it is even
possible to have callback between slice change.

The example below will print dataset \texttt{jobid} for each dataset
prior to iterating over it.
\begin{python}
# pre_callback once per dataset
def prefun(jobid):
    print(jobid)
for user, item in datasets.source.iterate_chain(sliceno, ('user', 'item',),
                                       pre_callback=prefun):
    ...
\end{python}
Next is an example of an iterator running over all slices.
The callback function is executed before each new slice is iterated.
Note the difference between this example and the previous.  The
callback function in this example takes two arguments, while the
previous takes only one.

\begin{python}
# callback once per slice
def prefun(jobid, sliceno):
    print(sliceno, jobid)
for user, item in datasets.source.iterate_chain(None, ('user', 'item',),
                                       pre_callback=prefun):
    ...
\end{python}
Post-callback is defined similarly.



\subsubsection*{Skipping Datasets and Slices from Callbacks}
It is possible to skip iterations by raising exceptions, as follows
\begin{python}

# Raise this in pre_callback to skip iterating the coming slice
# (if your callback doesn't want sliceno, this is the same as SkipJob)
raise SkipSlice

# Raise this in pre_callback to skip iterating the coing job
raise SkipJob

# Raise this to quit iterating, but with the side effect that
# post_callback will not run.
# @@@ TO BE DEFINED
raise StopIteration
\end{python}
