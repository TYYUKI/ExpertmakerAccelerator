%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Iterators are members of the \texttt{dataset} class.  They are
available in methods for streaming dataset data one row at a time into
a program.


\section{Basic Iteration}

The iterator iterates over one or more specified data columns.  Each
output is a tuple corresponding to the specified columns for one data
row.  In case of iterating over a single column, the output may
optionally be a scalar for more efficient computing.  Iterators can be
parallel, in \analysis, or sequential, in \prepare or \synthesis.

There are two iterators available, one for single dataset iteration
(called \texttt{iterate}), and one for iterating over dataset chains
(called (\texttt{iterate\_chain}).  Both iterators share these arguments
\begin{snugshade}
\begin{tabular}{llp{6cm}}

  \texttt{sliceno} & \textsl{mandatory} & Slice number to iterate
  over, or \mintinline{python}/None/ to iterate over all slices
  sequenctially. \\

  \texttt{columns} & None & Tuple of column labels or a single name if
  iterating over one column.\\

  \texttt{hashlabel} & None & Name of hash column.  If dataset is not
  hashed, the iterator will re-hash on-the-fly.\\

  \texttt{filters} & None & Filters decide which rows to include.
  Explained in section~\ref{xx}\\

  \texttt{translators} & None & Translators transform data values.
  Explained in section~\ref{xx}\\
  
  \texttt{status\_reporting} & True & Give status when pressing
  \texttt{C-t}.  Unless manually \texttt{zip}ing iterators, this
  should be set to default \mintinline{python}/True/.  See
  \texttt{dataset.py} for full information.\\
\end{tabular}
\end{snugshade}
In addition, \texttt{iterate\_chain} takes these arguments too
\begin{snugshade}
\begin{tabular}{lll}
  \texttt{length}&&\\
  \texttt{range}&&\\
  \texttt{sloppy\_range}&&\\
  \texttt{reverse}&&\\
  \texttt{stop\_ds}&&\\
  \texttt{pre\_callback}&&\\
  \texttt{post\_callback}&&\\
\end{tabular}
\end{snugshade}



\subsection*{Parallel Iterator Invocation}
For parallel iteration in \analysis, the iterator needs to know the
number of the current slice.  The following is an example of iteration
that happens independently in each slice.
\comment{speca sliceno och labels - argumenten}
\begin{python}
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate(sliceno, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
The program creates dictionaries mapping \texttt{user}s to sets of
\texttt{item}s for the \texttt{source} dataset.  (If we assume that
the dataset is hashed~\ref{xx}, this operation is entirely parallel
and there is no need to merge all the results from the analysis
processes later.

\subsection*{Sequential Iteratior Invocation}
By specifying the \texttt{sliceno} parameter to
\mintinline{python}/None/, the iterator will run through all slices of
the dataset, one at a time, like in this example
\begin{python}
def synthesis():
    h = defaultdict(set)
    for user, item in datasets.source.iterate(None, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
Slices will be iterated one at at time in increasing order.




%% "''Iterator over the specified columns from datasets
%% (iterable of dataset-specifiers, or single dataset-specifier).
%% callbacks are called before and after each dataset is iterated.

%% filters decide which rows to include and can be a callable
%% (called with the candidate tuple), or a dict {name: filter}.
%% In the latter case each individual filter is called with the column
%% value, or if it's None uses the column value directly.
%% All filters must say yes to yield a row.
%% examples:
%% filters={'some_col': some_dict.get}
%% filters={'some_col': some_set.__contains__}
%% filters={'some_col': some_str.__eq__}
%% filters=lambda line: line[0] == line[1]

%% translators transform data values. It can be a callable (called with the
%% candidate tuple and expected to return a tuple of the same length) or a
%% dict {name: translation}.
%% Each translation can be a function (called with the column value and
%% returning the new value) or dict. Items missing in the dict yield None,
%% which can be removed with filters={'col': None}.

%% Translators run before filters.

%% You can also pass a single name (a str) as columns, in which case you
%% don't get a tuple back (just the values). Tuple-filters/translators also
%% get just the value in this case (column versions are unaffected).

%% If you pass a false value for columns you get all columns in name order.

%% range limits which rows you see. Specify {colname: (start, stop)} and
%% only rows where start <= colvalue < stop will be returned.
%% If you set sloppy_range=True you may get all rows from datasets that
%% contain any rows you asked for. (This can be faster.)

%% status_reporting should normally be left as True, which will give you
%% information about this iteration in ^T, but there is one case where you
%% need to turn it off:
%% If you manually zip a bunch of iterators, only one should do status
%% reporting. (Otherwise it looks like you have nested iteration in ^T,
%% and you will get warnings about incorrect ending order of statuses.)

%% def iterate(self, sliceno, columns=None, hashlabel=None, filters=None, translators=None, status_reporting=True):


\subsection*{Special Cases, Iterating Over All or a Singe Column}
It is possible to iterate over all columns in a dataset by specifying
an empty list of column names, like this
\begin{python}
for items in dataset.source.iterate(sliceno, None):
    print(items)  # is a tuple of all columns
\end{python}
The iterator will output a tuples populated with all column values for
each row.  The columns will be in sorted column name order.

Iterators output tuples, but in the case of iterating over a single
column it is more efficient to output scalars instead.  Here are the
two different ways to iterate over a single column
\begin{python}
# alternative 1, use lists/tuples
for user in datasets.source.iterate(sliceno, ('USER',)):
    userset.add(user[0])  # user is a tuple

# alternative 2, specify column as string, not list
for user in datasets.source.iterate(sliceno, 'USER',):
    userset.add(user)
\end{python}
Both styles are supported by filters and translators introduced later
i this chapter.




\subsection{Halting Iteration}

Iteration over a dataset chain will continue until all data is
exhausted.  Sometimes, one wish to limit the number of iterations.
There are several mechanisms for that, and they may be combined in the
same expression.  If so, iteration will be over the shortest range of
the conditions.

\subsubsection*{Halting using length}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    length = options.length):
\end{python}
This will iterate for options.length number of datasets.  Note that a
length of -1 iterates without bounds and is the default.


\subsubsection*{Halting using stop\_jobid}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    stop_jobid = datasets.stopjob):
\end{python}

A more advanced, but very useful, method is to stop at a dataset that
is input to another job

\subsubsection*{Halting using another job's input parameters}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    stop_jobid = {jobids.preprocess: 'source',}):
\end{python}
this will iterate until reaching end or the dataset
job\_params(jobids.preprocess).datasets.source.

It is also possible to iterate over a specified data range.  This
works for datasets that are sorted on the column of choice.

\subsubsection*{Iterating over a data range}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    range={timestamp, ('2016-01-01', '2016-01-31'),}):
\end{python}
this will limit the iterator to exactly the range of lines that
fulfill the range condition.  It is relatively costly to filter each
line, and there is a speed advantage by specifying sloppy\_range,
which will iterate over all datasets that contain part of the range.

\subsection{Iterating in the reverse direction}

It is possible to iterate the dataset chain in the backwards direction
by specifying reverse=True, but note that iteration is always in the
forward direction within each dataset.

\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    reverse=True):
\end{python}

@@@ HASHLABEL FOR AVOIDING MISTAKES!!!

\subsection{iterate\_list}
PLACEHOLDER
%\begin{python}
%  # def iterate_list(sliceno, columns, jobids, hashlabel=None, pre_callback=None, post_callback=None, filters=None, translators=None):
%  from dataset import Dataset
%  for item in Dataset.iterate_list(None, None, datasets.source):
%    print(item)
%\end{python}


\subsection{Translators}

Translators transform data values. A translator is either a callable or
a \texttt{dict}.  Translators are similar to \texttt{filters}, and
always executed before filtering.

The idea behind translators and filters is that they provide a way to
modify code behavior by supplying functions as iterator options.  In
this way, it is possible to write re-useable simple methods that still
could be altered significantly in behavior.



\subsubsection*{Callable Translator}

A translator function is called with a tuple and is expected to return
a tuple of the same length.  Using translator functions it is possible
to mix different columns with each other before sending them to the
iterator output.  Here is an example

\begin{python}
def merger(user, item):
    return "%s:%s" % (user, item), None
for merge, _ in iterate_datasetchain(None, ('user', 'item',), tip_jobid=jobid,
                                     translator=merger):
    ...
\end{python}
The purpose of this translator is to convert each
\texttt{(user, item)} tuple to a string \texttt{user:item}.  This is
the first output of the translator and iterator and is stored in the
\texttt{merge} variable.  The second output variable is not used in
this application, but a variable still has to be assigned.



\subsubsection*{Translator dict}

One or more columns may be translated independently using a translator
dictionary, specified as \texttt{\{name:\ translation\}}.  A
translation may be either a \texttt{dict} or a callable.  Examples of
both kinds are shown below.

First an example illustrating the use of a translation \texttt{dict}.
Here, integers are translated into more comprehensible strings.

\begin{python}
mapper = {2: 'HUMANLIKE', 4: 'LABRADOR', 5: 'STARFISH',}
for animal in iterate_datasetchain(None, 'NUM_LEGS', tip_jobid=jobid,
                                     translator={'NUM_LEGS': mapper,}):
    ...
\end{python}
Items missing in a translation-\texttt{dict} yield \texttt{None}.

Second, an example of a translation callable.  In this example, each
user string is output from the iterator reads backwards.

\begin{python}
def reverse(x):
    return x[::-1]
for resu, item in iterate_datasetchain(None, ('USER', 'ITEM',), tip_jobid=jobid,
                                       translator={'USER': reverse,}):
    ...
\end{python}



%\newpage
\subsection{Filters}

Filters are run \emph{after} translators.

Filters decide which rows to pass to the iterator output.  A filter is
either a callable or a \texttt{dict}.


\subsubsection*{Callable Filter}

Callable filters receive the iterator tuple as input.  The output must
be \texttt{True} for the tuple to be output from the iterator,
otherwise the iterator continues reading the next line.

The following two examples, of which the first uses a callable filter,
are functionally equivalent

\begin{python}
# Ex1.  filter users that are bearded and longhaired
filter=lambda line: line[1] and line[2]
for user, beard, hair in iterate_datasetchain(None, ('user', 'bearded', 'longhaired'),
                                       tip_jobid=jobid, filters=filter):
    ...

# Ex2.  filter users that are bearded and longhaired
for user, beard, hair in iterate_datasetchain(None, ('user', 'bearded', 'longhaired'),
                                       tip_jobid=jobid):
    if beard and hair:
        ...
\end{python}




\subsubsection*{Filter Dict}

It is possible to filter on one or more columns independently using a
\texttt{dict}.  All filters must be \texttt{True} for a line to be
output from the iterator.  Here are two examples.  The first example
will remove all lines except the ones with valid users.  The second
example will only keep lines with movie items.  (The fact that book is
false is redundant --- missing keys will result in discarded lines.)

\begin{python}
# keep valid users only
validusers={'user1', 'user2', 'user3'}
filters={'user': validusers.__contains__}

# keep valid users with movie items
validitems={'movie': True, 'book': False}
filters={'user': validusers.__contains__, 'item': validitems.get}
\end{python}




\subsubsection*{Filter by Column Values}


Column values could also be used directly, i.e.\ the values get
evaluated by Python.  For example, assume there is a column
\texttt{hasbeard} with values being boolean integers, i.e.\ \texttt{1}
or \texttt{0}.  Bearded users may be collected by
\begin{python}
for user, beard in iterate_datasetchain(None, ('user', 'bearded',), tip_jobid=jobid,
                                       filters={'bearded': None}):
    bearded.add(user)
\end{python}
This may seem strange at first, but it works because the key
for the \texttt{bearded} column exists, and the value is
\texttt{None}, and not a callable nor a \texttt{dict}.




%You can also pass a single name (a str) as names_list, in which case
%you don't get a tuple back (just the
%values). Tuple-filters/translators also get just the value in this
%case (column versions are unaffected).

    


\subsection{Callback}
\label{sec:callback}

It is also possible to inject callback functions, either before or
after each dataset is loaded.  If \texttt{sliceno=None},
i.e.\ iteration runs over all slices of all datasets, it is even
possible to have callback between slice change.

The example below will print dataset \texttt{jobid} for each dataset
prior to iterating over it.
\begin{python}
# pre_callback once per dataset
def prefun(jobid):
    print(jobid)
for user, item in iterate_datasetchain(sliceno, ('user', 'item',), tip_jobid=jobid,
                                       pre_callback=prefun):
    ...
\end{python}
Next is an example of an iterator running over all slices.
The callback function is executed before each new slice is iterated.
Note the difference between this example and the previous.  The
callback function in this example takes two arguments, while the
previous takes only one.

\begin{python}
# callback once per slice
def prefun(jobid, sliceno):
    print(sliceno, jobid)
for user, item in iterate_datasetchain(None, ('user', 'item',), tip_jobid=jobid,
                                       pre_callback=prefun):
    ...
\end{python}
Post-callback is defined similarly.



\subsubsection*{Skipping Datasets and Slices from Callbacks}
It is possible to skip iterations by raising exceptions, as follows
\begin{python}

# Raise this in pre_callback to skip iterating the coming slice
# (if your callback doesn't want sliceno, this is the same as SkipJob)
raise SkipSlice

# Raise this in pre_callback to skip iterating the coing job
raise SkipJob

# Raise this to quit iterating, but with the side effect that
# post_callback will not run.
# @@@ TO BE DEFINED
raise StopIteration
\end{python}
